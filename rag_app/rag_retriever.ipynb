{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-05T22:45:10.698529Z",
     "start_time": "2025-11-05T22:45:10.062450Z"
    }
   },
   "source": [
    "import bs4\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from rag_app.llm import embeddings, llm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:45:10.783906Z",
     "start_time": "2025-11-05T22:45:10.703010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "print(docs[0].page_content[:500])"
   ],
   "id": "107be32451009e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:46:39.780788Z",
     "start_time": "2025-11-05T22:46:35.996616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ],
   "id": "75aa8d1d830b93a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n",
      "['55631d24-3cf7-4d47-b8b7-9f72bcd72c0b', '9b77c76b-eafb-4063-a61d-ea59bbca3f63', 'd2e9119a-e604-4f9a-914c-ab682c86dbcd']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:46:35.914427Z",
     "start_time": "2025-11-05T22:45:14.660492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "\n",
    "agent = create_agent(llm, tools=tools, system_prompt=prompt)\n",
    "\n",
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "id": "ed15eb7002890443",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (10e1c2fa-e96b-4a66-a2fd-41961d06aca6)\n",
      " Call ID: 10e1c2fa-e96b-4a66-a2fd-41961d06aca6\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (3f39fa3d-fb2e-48fa-95ef-ffdcc10bdd21)\n",
      " Call ID: 3f39fa3d-fb2e-48fa-95ef-ffdcc10bdd21\n",
      "  Args:\n",
      "    query: Tree of Thoughts extension to CoT common extensions\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 40518}\n",
      "Content: Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "**Standard method for Task Decomposition**\n",
      "\n",
      "The most widely‑adopted prompting technique for breaking a complex task into smaller, manageable steps is **Chain of Thought (CoT)**.  \n",
      "- CoT asks the model to “think step‑by‑step,” thereby leveraging more test‑time computation and revealing the model’s internal reasoning process.  \n",
      "- In practice a prompt might look like:  \n",
      "  ```\n",
      "  Q: What are the steps to bake a cake?  \n",
      "  A: 1. Gather ingredients…\n",
      "  ```\n",
      "\n",
      "*(Source: Lilian Weng, “LLM‑powered Autonomous Agents,” 2023 – see section “Task Decomposition”)*\n",
      "\n",
      "---\n",
      "\n",
      "**Common extensions of CoT**\n",
      "\n",
      "| Extension | What it adds to CoT | How it works (brief) | Key references |\n",
      "|-----------|--------------------|----------------------|----------------|\n",
      "| **Tree of Thoughts (ToT)** | Explores multiple reasoning branches at each step, forming a tree rather than a single linear chain. | After decomposing the problem into a few “thought steps,” the model generates several *thoughts* per step. A search strategy (BFS/DFS) traverses the tree, evaluating each node via a classifier or majority vote. | Yao et al., “Tree of Thoughts” (arXiv 2305.10601) |\n",
      "| **ReAct** | Couples reasoning (CoT) with acting (calling tools/ APIs) in a single loop. | The model alternates between “thinking” and “acting” steps, allowing it to fetch external knowledge or execute actions while reasoning. | Yao et al., “ReAct: Synergizing Reasoning and Acting” (ICLR 2023) |\n",
      "| **Self‑Reflection** | Adds a post‑processing loop where the model reviews and possibly revises its own chain of thought. | After generating an answer, the model is prompted to reflect on the reasoning, identify mistakes, and produce a corrected chain or answer. | Discussed in the “Self‑Reflection” component of the same blog |\n",
      "| **LLM+P (Planning with Classical Planners)** | Offloads long‑horizon planning to an external planner, using PDDL as an intermediate language. | The LLM first translates the problem into a PDDL description, invokes a classical planner to produce a plan, then converts the plan back into natural language. | Liu et al., “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” (arXiv 2304.11477) |\n",
      "\n",
      "> **TL;DR:**  \n",
      "> • **Standard method:** *Chain of Thought (CoT)*.  \n",
      "> • **Key extensions:** *Tree of Thoughts (multiple reasoning branches)*, *ReAct* (reasoning + acting loop), *Self‑Reflection* (model‑review of its own chain), and *LLM+P* (delegating planning to an external planner).\n",
      "\n",
      "These extensions build on the core idea of CoT—decomposing a task into incremental reasoning steps—but add branching, tool usage, introspection, or external planning to handle more complex or structured problems.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
